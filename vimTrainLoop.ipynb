{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44b9d342-99e4-46cc-8b94-d59a3629dae5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys, os, pickle\n",
    "\n",
    "sys.path.insert(0, '/home/jupyter/AdversarialRobustness')\n",
    "sys.path.insert(0, '/home/jupyter/AdversarialRobustness/vim')\n",
    "\n",
    "# Add sbin to path for Trigon - fix of common Debian error\n",
    "original_path = os.environ.get('PATH')\n",
    "os.environ['PATH'] = original_path + ':/sbin'\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from PIL import Image\n",
    "import requests\n",
    "from matplotlib import pyplot as plt\n",
    "from torchvision import datasets\n",
    "from transformers import ViTImageProcessor, ViTForImageClassification\n",
    "\n",
    "from timm.data.constants import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD\n",
    "\n",
    "import loadVim\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "861ffc60-8649-4e95-85d7-7cd8eb1573f2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def save_checkpoint(state, filename=\"checkpoint.pth.tar\"):\n",
    "    torch.save(state, filename)\n",
    "\n",
    "def load_checkpoint(filename=\"checkpoint.pth.tar\", device=torch.device('cuda')):\n",
    "    checkpoint = torch.load(filename, map_location=device)\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "    epoch = checkpoint['epoch']\n",
    "    return model, optimizer, epoch, checkpoint.get('loss', 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "33fa575f-0233-4a55-82c9-7b9e0e5873ef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_accuracy(outputs, labels):\n",
    "    _, preds = torch.max(outputs, 1) \n",
    "    correct = torch.sum(preds == labels).item() \n",
    "    return correct / len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "78cc0ffc-45f7-4d2d-9b4c-b56de813bd8b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def validate(model, valid_loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_accuracy = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in valid_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs.logits, labels)\n",
    "\n",
    "            accuracy = compute_accuracy(outputs.logits, labels)\n",
    "            total_accuracy += accuracy\n",
    "            total_loss += loss.item()\n",
    "    \n",
    "    average_loss = total_loss / len(valid_loader)\n",
    "    average_accuracy = total_accuracy / len(valid_loader)\n",
    "    return average_loss, average_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6060a5d-7ed2-427b-ae9c-814be851466b",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "58e1973c-22f4-44d2-a02d-0621a17e114c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train(model, train_loader, valid_loader, optimizer, criterion, device, num_epochs, pathmodel):\n",
    "    logs = []\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        total_accuracy = 0\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs.logits, labels)\n",
    "\n",
    "            accuracy = compute_accuracy(outputs.logits, labels)\n",
    "            total_accuracy += accuracy\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        average_loss = total_loss / len(train_loader)\n",
    "        average_accuracy = total_accuracy / len(train_loader)\n",
    "        val_loss, val_accuracy = validate(model, valid_loader, criterion, device)\n",
    "        \n",
    "        logs.append({\"train\":{\"loss\":average_loss, \"acc\":average_accuracy}, \"val\":{\"loss\":val_loss, \"acc\":val_accuracy}})\n",
    "\n",
    "        print(f\"Epoch {epoch+1}, Train Loss: {average_loss:.4f}, Train Accuracy: {average_accuracy:.4f}, Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}\")\n",
    "    save_checkpoint({'state_dict': model.vim_model.state_dict(), 'log':logs}, filename=pathmodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c12cf804-236f-4cf3-9446-b63733cf03a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainWrap(pathtrain, pathval, pathmodel, nepochs):\n",
    "    model, processor = loadVim.prepareDownstreamModel()\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "        lambda image: processor(images=image, return_tensors='pt')['pixel_values'].squeeze(0)\n",
    "    ])\n",
    "\n",
    "    datasetTrain = datasets.ImageFolder(pathtrain, transform = transform)\n",
    "    datasetVal = datasets.ImageFolder(pathval, transform = transform)\n",
    "\n",
    "    loaderTrain = DataLoader(datasetTrain, batch_size=128, shuffle=True)\n",
    "    loaderVal = DataLoader(datasetVal, batch_size=128, shuffle=False)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    model = model.to(device)\n",
    "    \n",
    "    train(model, loaderTrain, loaderVal, optimizer, criterion, device, nepochs, pathmodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c63efafa-b2cb-4c95-9c33-82250c83d248",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss: 1.9095, Train Accuracy: 0.2878, Val Loss: 1.3994, Val Accuracy: 0.4831\n",
      "Epoch 2, Train Loss: 1.3237, Train Accuracy: 0.5229, Val Loss: 1.1946, Val Accuracy: 0.5733\n",
      "Epoch 3, Train Loss: 1.1047, Train Accuracy: 0.5853, Val Loss: 1.1789, Val Accuracy: 0.5279\n",
      "Epoch 4, Train Loss: 1.0166, Train Accuracy: 0.6400, Val Loss: 1.0358, Val Accuracy: 0.6000\n",
      "Epoch 5, Train Loss: 0.8842, Train Accuracy: 0.6776, Val Loss: 1.0370, Val Accuracy: 0.5647\n",
      "Epoch 6, Train Loss: 0.8506, Train Accuracy: 0.6917, Val Loss: 0.9461, Val Accuracy: 0.6748\n",
      "Epoch 7, Train Loss: 0.7604, Train Accuracy: 0.7245, Val Loss: 0.9037, Val Accuracy: 0.6864\n",
      "Epoch 8, Train Loss: 0.7960, Train Accuracy: 0.6949, Val Loss: 1.0110, Val Accuracy: 0.6186\n",
      "Epoch 9, Train Loss: 0.7697, Train Accuracy: 0.7079, Val Loss: 0.9418, Val Accuracy: 0.6660\n",
      "Epoch 10, Train Loss: 0.7355, Train Accuracy: 0.7115, Val Loss: 0.9198, Val Accuracy: 0.6702\n",
      "Epoch 11, Train Loss: 0.7267, Train Accuracy: 0.7364, Val Loss: 0.9789, Val Accuracy: 0.6483\n",
      "Epoch 12, Train Loss: 0.6252, Train Accuracy: 0.7747, Val Loss: 0.9064, Val Accuracy: 0.6683\n",
      "Epoch 13, Train Loss: 0.6202, Train Accuracy: 0.7908, Val Loss: 0.8623, Val Accuracy: 0.6913\n",
      "Epoch 14, Train Loss: 0.5880, Train Accuracy: 0.8186, Val Loss: 0.8541, Val Accuracy: 0.6828\n",
      "Epoch 15, Train Loss: 0.5616, Train Accuracy: 0.8257, Val Loss: 0.9347, Val Accuracy: 0.6663\n",
      "Epoch 16, Train Loss: 0.5947, Train Accuracy: 0.7889, Val Loss: 0.8134, Val Accuracy: 0.7197\n",
      "Epoch 17, Train Loss: 0.5818, Train Accuracy: 0.7977, Val Loss: 0.8458, Val Accuracy: 0.6962\n",
      "Epoch 18, Train Loss: 0.5378, Train Accuracy: 0.7972, Val Loss: 0.8340, Val Accuracy: 0.6710\n",
      "Epoch 19, Train Loss: 0.5188, Train Accuracy: 0.8218, Val Loss: 0.8280, Val Accuracy: 0.6827\n",
      "Epoch 20, Train Loss: 0.5184, Train Accuracy: 0.8367, Val Loss: 0.8502, Val Accuracy: 0.6660\n"
     ]
    }
   ],
   "source": [
    "pathtrain = \"/home/jupyter/AdversarialRobustness/CelebSubset/CelebTrainA\"\n",
    "pathval = \"/home/jupyter/AdversarialRobustness/CelebSubset/CelebVal\"\n",
    "pathmodel = \"/home/jupyter/AdversarialRobustness/models/VimA.pth\"\n",
    "\n",
    "trainWrap(pathtrain, pathval, pathmodel, 20)\n",
    "# state_dict and log saved in dictionary at pathmodel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "caa8edac-85e7-4e94-a4af-91284cba5f0f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss: 1.8283, Train Accuracy: 0.3146, Val Loss: 1.3761, Val Accuracy: 0.5262\n",
      "Epoch 2, Train Loss: 1.2348, Train Accuracy: 0.5428, Val Loss: 1.1845, Val Accuracy: 0.5520\n",
      "Epoch 3, Train Loss: 0.9911, Train Accuracy: 0.6424, Val Loss: 1.0807, Val Accuracy: 0.5804\n",
      "Epoch 4, Train Loss: 0.8882, Train Accuracy: 0.6588, Val Loss: 1.0678, Val Accuracy: 0.5999\n",
      "Epoch 5, Train Loss: 0.8155, Train Accuracy: 0.7176, Val Loss: 1.0476, Val Accuracy: 0.6122\n",
      "Epoch 6, Train Loss: 0.7544, Train Accuracy: 0.7310, Val Loss: 0.9935, Val Accuracy: 0.6220\n",
      "Epoch 7, Train Loss: 0.6427, Train Accuracy: 0.7776, Val Loss: 1.0437, Val Accuracy: 0.6252\n",
      "Epoch 8, Train Loss: 0.6508, Train Accuracy: 0.7823, Val Loss: 0.9869, Val Accuracy: 0.6415\n",
      "Epoch 9, Train Loss: 0.6308, Train Accuracy: 0.7908, Val Loss: 1.0071, Val Accuracy: 0.6095\n",
      "Epoch 10, Train Loss: 0.5679, Train Accuracy: 0.8289, Val Loss: 1.0041, Val Accuracy: 0.6435\n",
      "Epoch 11, Train Loss: 0.5906, Train Accuracy: 0.8062, Val Loss: 0.9721, Val Accuracy: 0.6318\n",
      "Epoch 12, Train Loss: 0.5398, Train Accuracy: 0.8250, Val Loss: 1.0112, Val Accuracy: 0.6271\n",
      "Epoch 13, Train Loss: 0.5756, Train Accuracy: 0.8067, Val Loss: 0.9817, Val Accuracy: 0.6387\n",
      "Epoch 14, Train Loss: 0.5286, Train Accuracy: 0.8094, Val Loss: 0.9785, Val Accuracy: 0.6212\n",
      "Epoch 15, Train Loss: 0.4791, Train Accuracy: 0.8377, Val Loss: 0.9460, Val Accuracy: 0.6503\n",
      "Epoch 16, Train Loss: 0.4733, Train Accuracy: 0.8435, Val Loss: 0.9586, Val Accuracy: 0.6473\n",
      "Epoch 17, Train Loss: 0.4752, Train Accuracy: 0.8343, Val Loss: 0.9182, Val Accuracy: 0.6709\n",
      "Epoch 18, Train Loss: 0.4480, Train Accuracy: 0.8479, Val Loss: 0.9843, Val Accuracy: 0.6387\n",
      "Epoch 19, Train Loss: 0.4481, Train Accuracy: 0.8509, Val Loss: 0.9331, Val Accuracy: 0.6581\n",
      "Epoch 20, Train Loss: 0.4342, Train Accuracy: 0.8582, Val Loss: 0.9411, Val Accuracy: 0.6582\n"
     ]
    }
   ],
   "source": [
    "pathtrain = \"/home/jupyter/AdversarialRobustness/CelebSubset/CelebTrainB\"\n",
    "pathval = \"/home/jupyter/AdversarialRobustness/CelebSubset/CelebVal\"\n",
    "pathmodel = \"/home/jupyter/AdversarialRobustness/models/VimB.pth\"\n",
    "\n",
    "trainWrap(pathtrain, pathval, pathmodel, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b29039e7-b22c-4f33-91d2-e693e304c4f7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for dataset  A\n",
      "(0.9388849437236786, 0.6850490196078431)\n",
      "Results for dataset  B\n",
      "(0.9405563771724701, 0.7212775735294117)\n"
     ]
    }
   ],
   "source": [
    "# Accuracies in hindsight:\n",
    "\n",
    "for dataset in [\"A\", \"B\"]:\n",
    "    mod = torch.load(f\"/home/jupyter/AdversarialRobustness/models/Vim{dataset}.pth\")\n",
    "    model, processor = loadVim.prepareDownstreamModel()\n",
    "    model.vim_model.load_state_dict(mod[\"state_dict\"])\n",
    "    transform = transforms.Compose([\n",
    "            lambda image: processor(images=image, return_tensors='pt')['pixel_values'].squeeze(0)\n",
    "        ])\n",
    "    datasetTest = datasets.ImageFolder(\"/home/jupyter/AdversarialRobustness/CelebSubset/CelebTest\", transform = transform)\n",
    "    loaderTrain = DataLoader(datasetTest, batch_size=128, shuffle=False)\n",
    "    print(\"Results for dataset \", dataset)\n",
    "    print(validate(model.to(device), loaderTrain, torch.nn.CrossEntropyLoss(), device))"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-env-pytorch-pytorch",
   "name": "workbench-notebooks.m121",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m121"
  },
  "kernelspec": {
   "display_name": "PyTorch 1-13 (Local)",
   "language": "python",
   "name": "conda-env-pytorch-pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
